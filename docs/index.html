<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight:300;
		font-size:17px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}

	h1 {
		font-weight:300;
        font-size: 30px;
	}

	a:link,a:visited {
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}

	.img-zoom {
		transition: transform .2s;
	}

	.img-zoom:hover {
		transform: scale(1.05);
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
		transition: transform .2s;
	}

	.layered-paper-big:hover {
		transform: scale(1.05);
	}

	hr {
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}

    table td, table td * {
        vertical-align: top;
    }
</style>

<html lang="en">
	<head>
		<meta charset="utf-8">

		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-M4J6410VJR"></script>
		<script>
			window.dataLayer = window.dataLayer || [];
			function gtag(){dataLayer.push(arguments);}
			gtag('js', new Date());

			gtag('config', 'G-M4J6410VJR');
		</script>

		<title>A Survey on Deep Domain Adaptation for LiDAR Perception</title>
		<link rel="icon" href="https://larissa.triess.eu/survey-uda-lidar/images/transfer_learning.svg">
		<meta property="og:image" content="https://larissa.triess.eu/survey-uda-lidar/images/transfer_learning.svg"/>
		<meta property="og:title" content="A Survey on Deep Domain Adaptation for LiDAR Perception"/>
		<meta property="og:description" content="Project page for 'A Survey on Deep Domain Adaptation for LiDAR Perception' with links to the paper."/>
		<meta property="og:keywords" content="domain adaptation, lidar, survey, machine learning, transfer learning, unsupervised, domain mapping"/>
		<meta property="og:author" content="Larissa Triess"/>
	</head>

	<body>
		<br>
		<!-------------------------------- HEADLINE -------------------------------->
		<div style="text-align: center;">
			<!-------------------------------- TITLE -------------------------------->
			<span style="font-size:36px">A Survey on Deep Domain Adaptation for LiDAR Perception</span><br><br>
			<br>

			<!-------------------------------- AUTHORS -------------------------------->
			<table style="width: 960px; text-align: center; margin-left:auto; margin-right:auto">
				<tr>
				<td>
					<span style="font-size:20px"><a href="https://larissa.triess.eu">Larissa T. Triess</a><sup>1,2</sup></span>
				</td>
				<td>
					<span style="font-size:20px"><a href="https://github.com/mella30">Mariella Dreissig</a><sup>1</sup></span>
				</td>
				<td>
					<span style="font-size:20px"><a href="https://github.com/risteon">Christoph B. Rist</a><sup>1</sup></span>
				</td>
				<td>
					<span style="font-size:20px"><a href="https://www.aifb.kit.edu/web/J._Marius_Z%C3%B6llner">J. Marius Zöllner</a><sup>2,3</sup></span>
				</td>
				</tr>
			</table>
			<br>

			<!-------------------------------- AFFILIATIONS -------------------------------->
			<table style="width: 960px; text-align: center; margin-left:auto; margin-right:auto">
				<tr>
				<td>
					<span style="font-size:18px"><sup>1</sup>Mercedes-Benz AG<br>Stuttgart (Germany)</span>
				</td>
				<td>
					<span style="font-size:18px"><sup>2</sup>Karlsruhe Institute of Technology<br>Karlsruhe (Germany)</span>
				</td>
				<td>
					<span style="font-size:18px"><sup>3</sup>Research Center for Information Technology<br>Karlsruhe (Germany)</span>
				</td>
				</tr>
			</table>
			<br>

			<!-------------------------------- CONFERENCE -------------------------------->
			<table style="width: 960px; text-align: center; margin-left:auto; margin-right:auto">
				<tr>
				<td>
					<span style="font-size:18px">In 2021 IEEE Intelligent Vehicles Symposium (IV)<br>Workshop on Autonomy at Scale</span>
				</td>
				</tr>
			</table>
			<br>

			<!-------------------------------- RESOURCES -------------------------------->
			<table style="width: 960px; text-align: center; margin-left:auto; margin-right:auto">
				<tr>
				<td>
					<span style="font-size:24px"><a href='https://arxiv.org/pdf/2106.02377.pdf'>[Paper]</a></span>
					<!-- &emsp;&emsp;
					<span style="font-size:24px"><a href='https://larissa.triess.eu/assets/pdf/triess2021iv_poster.pdf'>[Poster]</a></span> -->
				</td>
				</tr>
			</table>
		</div>

		<br>
		<br>
		<!-------------------------------- TEASER IMAGE -------------------------------->
		<table style="width: 960px; text-align: center; margin-left:auto; margin-right:auto">
            <tr>
                <td style="padding: 10px">
                    <a href="images/transfer_learning.svg">
                        <img class="img-zoom" src="images/transfer_learning.svg" width="60%" alt="transferlearning"/>
                    </a>
                </td>
            </tr>
		</table>

		<table style="width: 960px; text-align: center; margin-left:auto; margin-right:auto">
			<tr>
				<center>
					<td style="font-size:14px">
						<i>
							<b>Overview of Transfer Learning</b>:<br>
							Domain adaptation is a type of transductive transfer learning where the same task is performed in different, but related domains with annotated data only in the source domain.
							(Figure adapted from [<a href='https://ieeexplore.ieee.org/document/5288526'>Pan2010</a>]).
						</i>
					</td>
				</center>
			</tr>
		</table>

		<br>
		<br>
		<hr><!-------------------------------- ABSTRACT -------------------------------->

		<div id="abstract" style="text-align: center;"><h1>Abstract</h1></div>

		<table style="width: 960px; text-align: left; margin-left:auto; margin-right:auto">
			<tr>
			<td>
				Scalable systems for automated driving have to reliably cope with an open-world setting.
				This means, the perception systems are exposed to drastic domain shifts, like changes in weather conditions, time-dependent aspects, or geographic regions.
				Covering all domains with annotated data is impossible because of the endless variations of domains and the time-consuming and expensive annotation process.
				Furthermore, fast development cycles of the system additionally introduce hardware changes, such as sensor types and vehicle setups, and the required knowledge transfer from simulation.
				<br><br>
				To enable scalable automated driving, it is therefore crucial to address these domain shifts in a robust and efficient manner.
				Over the last years, a vast amount of different domain adaptation techniques evolved.
				There already exists a number of survey papers for domain adaptation on camera images, however, a survey for LiDAR perception is absent.
				Nevertheless, LiDAR is a vital sensor for automated driving that provides detailed 3D scans of the vehicle's surroundings.
				To stimulate future research, this paper presents a comprehensive review of recent progress in domain adaptation methods and formulates interesting research questions specifically targeted towards LiDAR perception.
			</td>
			</tr>
		</table>

		<br>
		<br>
		<hr><!-------------------------------- PAPER -------------------------------->
		<div id="paper" style="text-align: center;"><h1>Paper</h1></div>

		<div style="text-align: center;">
			<span><a href="https://arxiv.org/abs/2106.02377"><img class="layered-paper-big" style="height:250px" src="./images/paper_thumb.png" alt=""/></a></span>
			<br><br><br><br>
			<span style="font-size:20pt">Paper: <a href="https://arxiv.org/abs/2106.02377">[ArXiv]</a> <!-- <a href="https://ieeexplore.ieee.org/document/TODO">[IEEEXplore]</a> --></span>
			<br><br>
			<span style="font-size:18pt">Citation: <a href="./resources/triess2021iv.bib">[BibTeX]</a></span>
		</div>

		<br>
		<br>
		<hr><!-------------------------------- CONTRIBUTIONS -------------------------------->

		<div id="contributions" style="text-align: center;"><h1>Contributions</h1></div>

		<table style="width: 960px; text-align: left; margin-left:auto; margin-right:auto">
			<tr>
			<td>
				The focus of this paper is to give an overview on DA methods that specifically address deep learning based LiDAR perception and discuss their unique features, use-cases, and challenges.
				The paper is organized as follows:
			</td>
			</tr>
			<tr>
			<td>
				Section 2 gives an introduction to common LiDAR perception tasks and the terminology of DA.
				The section also includes an overview on typical baselines, datasets, DA applications, and metrics.
				Section 3 categorizes common DA approaches for LiDAR.
				In Section 4, we discuss different aspects of the presented approaches and give an outlook on interesting research directions.
			</td>
			</tr>
		</table>

		<br>
		<br>
		<br>
		<br>
		<hr><!-------------------------------- METHODS -------------------------------->

		<div id="methods" style="text-align: center;"><h1>Methods</h1></div>

		<!-------------------------------- METHODS: Domain-Invariant Data Representation -------------------------------->
		<div id="methods" style="text-align: center;"><h3>Domain-Invariant Data Representations</h3></div>

		<table style="width: 960px; text-align: left; margin-left:auto; margin-right:auto">
			<tr>
				<td>
					A domain-invariant representation is a hand-crafted approach to move different domains into a common representation.
					Figure 3 shows that this approach is basically a data pre-processing after which a regular perception pipeline starts.
					It is mostly used to account for the sensor-to-sensor domain shift and receives special attention in LiDAR research.
					Available sensors vary in their resolution and sampling patterns while resulting point clouds are additionally influenced by the mounting position and the recording rate of the sensor.
					Consequently, the acquired data vary considerably in their statistics and distributions.
					This data distribution mismatch makes it unfeasible to apply the same model to different sensors in a naive way.
					Therefore, many simple DA methods either align the sampling differences in 2D space or use representations in 3D space that are less prone to domain differences.
				</td>
			</tr>
		</table>

		<br>

		<table style="width: 960px; text-align: center; margin-left:auto; margin-right:auto">
            <tr>
                <td style="padding: 10px">
                    <a href="images/method_invariant_data_train.svg">
                        <img class="img-zoom" src="images/method_invariant_data_train.svg" width="70%" alt="invariantdatatrain"/>
                    </a>
                </td>
            </tr>
            <tr>
				<td style="font-size:14px">
					(a) Training
				</td>
            </tr>
            <tr>
                <td style="padding: 10px">
                    <a href="images/method_invariant_data_test.svg">
                        <img class="img-zoom" src="images/method_invariant_data_test.svg" width="70%" alt="invariantdatatest"/>
                    </a>
                </td>
            </tr>
			<tr>
				<td style="font-size:14px">
					(b) Testing
				</td>
			</tr>
			<tr>
				<center>
					<td style="font-size:14px">
						<i>
							<b>Figure 3</b>:
							The data from the source domain at train-time (a) and the data from the target domain at test-time (b) are both converted into a hand-crafted common representation prior to being fed to the perception pipeline.
						</i>
					</td>
				</center>
			</tr>
		</table>

		<!-------------------------------- METHODS: Domain Mapping -------------------------------->
		<div id="methods" style="text-align: center;"><h3>Domain Mapping</h3></div>

		<table style="width: 960px; text-align: left; margin-left:auto; margin-right:auto">
			<tr>
				<td>
					Domain mapping aims at transferring the data of one domain to another domain and is most often used in sim-to-real and dataset-to-dataset applications.
					Figure 4 shows a typical setup for domain mapping.
					Annotated source data is usually transformed to appear like target data, creating a labeled pseudo-target dataset.
					With the transformed data, a perception network is trained which can then be applied to target data at test time.
					For images, domain mapping is usually done adversarially and at pixel-level in the form of image-to-image translation with conditional GANs.
					Similar principles apply to LiDAR data, however, there also exists a number of methods that do not rely on adversarial training.
				</td>
			</tr>
		</table>

		<br>

		<table style="width: 960px; text-align: center; margin-left:auto; margin-right:auto">
            <tr>
                <td style="padding: 10px">
                    <a href="images/method_domain_mapping_train.svg">
                        <img class="img-zoom" src="images/method_domain_mapping_train.svg" width="70%" alt="domainmappingtrain"/>
                    </a>
                </td>
            </tr>
            <tr>
				<td style="font-size:14px">
					(a) Training
				</td>
            </tr>
            <tr>
                <td style="padding: 10px">
                    <a href="images/method_domain_mapping_test.svg">
                        <img class="img-zoom" src="images/method_domain_mapping_test.svg" width="42%" alt="domainmappingtest"/>
                    </a>
                </td>
            </tr>
			<tr>
				<td style="font-size:14px">
					(b) Testing
				</td>
			</tr>
            <tr>
				<center>
					<td style="font-size:14px">
						<i>
							<b>Figure 4</b>:
							This is the most commonly used configuration for domain mapping.
							During training (a) the labeled source data is conditionally (dashed line) mapped to the target domain where a perception network is trained.
							At test time (b), the trained perception network can directly be applied to the target data.
						</i>
					</td>
				</center>
			</tr>
		</table>

		<!-------------------------------- METHODS: Domain-Invariant Feature Learning -------------------------------->
		<div id="methods" style="text-align: center;"><h3>Domain-Invariant Feature Learning</h3></div>

		<table style="width: 960px; text-align: left; margin-left:auto; margin-right:auto">
			<tr>
				<td>
					State-of-the-art methods in domain-invariant feature learning employ a training procedure that encourages the model to learn a feature representation that is independent of the domain.
					This is done by finding or constructing a common representation space for the source and target domain.
					In contrast to domain-invariant data representations, these approaches are not hand-crafted but use learned features.
					If the classifier model performs well on the source domain using a domain-invariant feature representation, then the classifier may generalize well to the target domain.
					The basic principle is depicted in Figure 5.
				</td>
			</tr>
		</table>

		<br>

		<table style="width: 960px; text-align: center; margin-left:auto; margin-right:auto">
            <tr>
                <td style="padding: 10px">
                    <a href="images/method_invariant_features_train.svg">
                        <img class="img-zoom" src="images/method_invariant_features_train.svg" width="70%" alt="invariantfeaturestrain"/>
                    </a>
                </td>
            </tr>
            <tr>
				<td style="font-size:14px">
					(a) Training
				</td>
            </tr>
            <tr>
                <td style="padding: 10px">
                    <a href="images/method_invariant_features_test.svg">
                        <img class="img-zoom" src="images/method_invariant_features_test.svg" width="70%" alt="invariantfeaturestest"/>
                    </a>
                </td>
            </tr>
			<tr>
				<td style="font-size:14px">
					(b) Testing
				</td>
			</tr>
            <tr>
				<center>
					<td style="font-size:14px">
						<i>
							<b>Figure 5</b>:
							A feature extractor network and an alignment component learn a domain-invariant feature encoding (a).
							At test time (b), the domain-invariant feature extractor is applied to the target data.
						</i>
					</td>
				</center>
			</tr>
		</table>

		<br>
		<br>
		<hr><!-------------------------------- ACKNOWLEDGEMENTS -------------------------------->

		<div id="acknowledgements" style="text-align: center;"><h1>Acknowledgements</h1></div>

		<table style="width: 960px; text-align: left; margin-left:auto; margin-right:auto">
			<tr>
			<td>
				This work was presented at the <i>Workshop on Autonomy at Scale</i> (WS52), IV2021.
				The research leading to these results is funded by the German Federal Ministry for Economic Affairs and Energy within the project "KI Delta Learning" (Förderkennzeichen 19A19013A).
			</td>
			</tr>
		</table>

		<br>
		<br>

	</body>
</html>
