<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight:300;
		font-size:17px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}

	h1 {
		font-weight:300;
        font-size: 30px;
	}

	a:link,a:visited {
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}

	.img-zoom {
		transition: transform .2s;
	}

	.img-zoom:hover {
		transform: scale(1.05);
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
		transition: transform .2s;
	}

	.layered-paper-big:hover {
		transform: scale(1.05);
	}

	hr {
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}

    table td, table td * {
        vertical-align: top;
    }
</style>

<html lang="en">
	<head>
		<meta charset="utf-8">

		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=G-M4J6410VJR"></script>
		<script>
			window.dataLayer = window.dataLayer || [];
			function gtag(){dataLayer.push(arguments);}
			gtag('js', new Date());

			gtag('config', 'G-M4J6410VJR');
		</script>

		<title>A Survey on Deep Domain Adaptation for LiDAR Perception</title>
		<link rel="icon" href="https://larissa.triess.eu/survey-uda-lidar/images/transfer_learning.svg">
		<meta property="og:image" content="https://larissa.triess.eu/survey-uda-lidar/images/transfer_learning.svg"/>
		<meta property="og:title" content="A Survey on Deep Domain Adaptation for LiDAR Perception"/>
		<meta property="og:description" content="Project page for 'A Survey on Deep Domain Adaptation for LiDAR Perception' with links to the paper."/>
		<meta property="og:keywords" content="domain adaptation, lidar, survey, machine learning, transfer learning, unsupervised, domain mapping"/>
		<meta property="og:author" content="Larissa Triess"/>
	</head>

	<body>
		<br>
		<!-------------------------------- HEADLINE -------------------------------->
		<div style="text-align: center;">
			<!-------------------------------- TITLE -------------------------------->
			<span style="font-size:36px">A Survey on Deep Domain Adaptation for LiDAR Perception</span><br><br>
			<br>

			<!-------------------------------- AUTHORS -------------------------------->
			<table style="width: 960px; text-align: center; margin-left:auto; margin-right:auto">
				<tr>
				<td>
					<span style="font-size:20px"><a href="https://larissa.triess.eu">Larissa T. Triess</a><sup>1,2</sup></span>
				</td>
				<td>
					<span style="font-size:20px"><a href="https://github.com/mella30">Mariella Dreissig</a><sup>1</sup></span>
				</td>
				<td>
					<span style="font-size:20px"><a href="https://github.com/risteon">Christoph B. Rist</a><sup>1</sup></span>
				</td>
				<td>
					<span style="font-size:20px"><a href="https://www.aifb.kit.edu/web/J._Marius_Z%C3%B6llner">J. Marius ZÃ¶llner</a><sup>2,3</sup></span>
				</td>
				</tr>
			</table>
			<br>

			<!-------------------------------- AFFILIATIONS -------------------------------->
			<table style="width: 960px; text-align: center; margin-left:auto; margin-right:auto">
				<tr>
				<td>
					<span style="font-size:18px"><sup>1</sup>Mercedes-Benz AG<br>Stuttgart (Germany)</span>
				</td>
				<td>
					<span style="font-size:18px"><sup>2</sup>Karlsruhe Institute of Technology<br>Karlsruhe (Germany)</span>
				</td>
				<td>
					<span style="font-size:18px"><sup>3</sup>Research Center for Information Technology<br>Karlsruhe (Germany)</span>
				</td>
				</tr>
			</table>
			<br>

			<!-------------------------------- CONFERENCE -------------------------------->
			<table style="width: 960px; text-align: center; margin-left:auto; margin-right:auto">
				<tr>
				<td>
					<span style="font-size:18px">In 2021 IEEE Intelligent Vehicles Symposium (IV)<br>Workshop on Autonomy at Scale</span>
				</td>
				</tr>
			</table>
			<br>

			<!-------------------------------- RESOURCES -------------------------------->
			<table style="width: 960px; text-align: center; margin-left:auto; margin-right:auto">
				<tr>
				<td>
					<span style="font-size:24px"><a href='https://arxiv.org/pdf/2106.02377.pdf'>[Paper]</a></span>
					<!-- &emsp;&emsp;
					<span style="font-size:24px"><a href='https://larissa.triess.eu/assets/pdf/triess2021iv_poster.pdf'>[Poster]</a></span> -->
				</td>
				</tr>
			</table>
		</div>

		<br>
		<br>
		<!-------------------------------- TEASER IMAGE -------------------------------->
		<table style="width: 960px; text-align: center; margin-left:auto; margin-right:auto">
            <tr>
                <td style="padding: 10px">
                    <a href="images/transfer_learning.svg">
                        <img class="img-zoom" src="images/transfer_learning.svg" width="60%" alt="transferlearning"/>
                    </a>
                </td>
            </tr>
		</table>

		<table style="width: 960px; text-align: center; margin-left:auto; margin-right:auto">
			<tr>
				<center>
					<td style="font-size:14px">
						<i>
							<b>Overview of Transfer Learning</b>:<br>
							Domain adaptation is a type of transductive transfer learning where the same task is performed in different, but related domains with annotated data only in the source domain.
							(Figure adapted from [<a href='https://ieeexplore.ieee.org/document/5288526'>Pan2010</a>]).
						</i>
					</td>
				</center>
			</tr>
		</table>

		<br>
		<br>
		<hr><!-------------------------------- ABSTRACT -------------------------------->

		<div id="abstract" style="text-align: center;"><h1>Abstract</h1></div>

		<table style="width: 960px; text-align: left; margin-left:auto; margin-right:auto">
			<tr>
			<td>
				Scalable systems for automated driving have to reliably cope with an open-world setting.
				This means, the perception systems are exposed to drastic domain shifts, like changes in weather conditions, time-dependent aspects, or geographic regions.
				Covering all domains with annotated data is impossible because of the endless variations of domains and the time-consuming and expensive annotation process.
				Furthermore, fast development cycles of the system additionally introduce hardware changes, such as sensor types and vehicle setups, and the required knowledge transfer from simulation.
				<br><br>
				To enable scalable automated driving, it is therefore crucial to address these domain shifts in a robust and efficient manner.
				Over the last years, a vast amount of different domain adaptation techniques evolved.
				There already exists a number of survey papers for domain adaptation on camera images, however, a survey for LiDAR perception is absent.
				Nevertheless, LiDAR is a vital sensor for automated driving that provides detailed 3D scans of the vehicle's surroundings.
				To stimulate future research, this paper presents a comprehensive review of recent progress in domain adaptation methods and formulates interesting research questions specifically targeted towards LiDAR perception.
			</td>
			</tr>
		</table>

		<br>
		<br>
		<hr><!-------------------------------- PAPER -------------------------------->
		<div id="paper" style="text-align: center;"><h1>Paper</h1></div>

		<div style="text-align: center;">
			<span><a href="https://arxiv.org/abs/2106.02377"><img class="layered-paper-big" style="height:250px" src="./images/paper_thumb.png" alt=""/></a></span>
			<br><br><br><br>
			<span style="font-size:20pt">Paper: <a href="https://arxiv.org/abs/2106.02377">[ArXiv]</a> <!-- <a href="https://ieeexplore.ieee.org/document/TODO">[IEEEXplore]</a> --></span>
			<br><br>
			<span style="font-size:18pt">Citation: <a href="./resources/triess2021iv.bib">[BibTeX]</a></span>
		</div>

		<br>
		<br>
		<hr><!-------------------------------- CONTRIBUTIONS -------------------------------->

		<div id="contributions" style="text-align: center;"><h1>Contributions</h1></div>

		<table style="width: 960px; text-align: left; margin-left:auto; margin-right:auto">
			<tr>
			<td>
				The focus of this paper is to give an overview on DA methods that specifically address deep learning based LiDAR perception and discuss their unique features, use-cases, and challenges.
				The paper is organized as follows:
			</td>
			</tr>
			<tr>
			<td>
				Section 2 gives an introduction to common LiDAR perception tasks and the terminology of DA.
				The section also includes an overview on typical baselines, datasets, DA applications, and metrics.
				Section 3 categorizes common DA approaches for LiDAR.
				In Section 4, we discuss different aspects of the presented approaches and give an outlook on interesting research directions.
			</td>
			</tr>
		</table>

		<br>
		<br>
		<br>
		<br>
		<hr><!-------------------------------- ACKNOWLEDGEMENTS -------------------------------->

		<div id="acknowledgements" style="text-align: center;"><h1>Acknowledgements</h1></div>

		<table style="width: 960px; text-align: left; margin-left:auto; margin-right:auto">
			<tr>
			<td>
				This work was presented at the <i>Workshop on Autonomy at Scale</i> (WS52), IV2021.
				The research leading to these results is funded by the German Federal Ministry for Economic Affairs and Energy within the project "KI Delta Learning" (FÃ¶rderkennzeichen 19A19013A).
			</td>
			</tr>
		</table>

		<br>
		<br>

	</body>
</html>
